{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b41b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1776cb1d",
   "metadata": {},
   "source": [
    "### Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9729250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(path):\n",
    "    files= os.listdir(path) \n",
    "    results = {'text':[], 'highlight': [], 'highlight_1':[], 'highlight_2':[], 'highlight_3':[], 'highlight_4':[]}\n",
    "    for file in tqdm.tqdm(files):\n",
    "        if not os.path.isdir(file):\n",
    "            file_name = path + '/'+file\n",
    "            with open(file_name, encoding=\"utf-8\") as f:\n",
    "                text = (f.read()).replace('\\n', \" \").replace(\"(CNN)\", \"\").replace(\"--\", \"\")\n",
    "                if len(text)<1000:\n",
    "                    continue\n",
    "                text_highlights = text.split(\"@highlight\")\n",
    "                final_text = text_highlights[0]\n",
    "                results['text'].append(final_text.strip())\n",
    "                all_highlight = \"\"\n",
    "                for i in range(1, 5):\n",
    "                    key = 'highlight_'+str(i)\n",
    "                    if i<len(text_highlights):\n",
    "                        results[key].append(text_highlights[i])\n",
    "                        all_highlight += text_highlights[i] + '.'\n",
    "                    else:\n",
    "                        results[key].append(\"\")\n",
    "                results['highlight'].append(all_highlight.strip())\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e3c59ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 3986.78it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 40000/40000 [00:06<00:00, 6565.10it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'train_data'\n",
    "test_dir = 'test'\n",
    "test_data = read_text(test_dir)\n",
    "train_data = read_text(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce305f48",
   "metadata": {},
   "source": [
    "### Set up vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d205acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pack_sequence, pad_packed_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter, OrderedDict\n",
    "from torchtext.vocab import vocab\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4525ec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "vocab_counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15e87669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_vocab(vocab_counters, text_dataframe):\n",
    "    tokens = {'text_tokens':[], 'highlight_tokens':[], 'tokens_num':[]}\n",
    "    for index, row in tqdm.tqdm(text_dataframe.iterrows()):\n",
    "        text_tokens = en_tokenizer(row['text'])\n",
    "        vocab_counters.update(text_tokens)\n",
    "        tokens['text_tokens'].append(text_tokens)\n",
    "        tokens['tokens_num'].append(len(text_tokens))\n",
    "        tokens['highlight_tokens'].append(en_tokenizer(row['highlight']))\n",
    "        \n",
    "    return pd.DataFrame(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68ad991b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39515it [02:04, 317.95it/s]\n",
      "1982it [00:05, 357.45it/s]\n"
     ]
    }
   ],
   "source": [
    "train_token = token_vocab(vocab_counter, train_data)\n",
    "test_token = token_vocab(vocab_counter, test_data)\n",
    "text_vocab = vocab(vocab_counter, min_freq = 2, specials=['<pad>','<unk>', '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1faa1317",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_train_df = train_token[train_token['tokens_num']>100].sort_values(by='tokens_num')\n",
    "sorted_test_df = test_token[test_token['tokens_num']>100].sort_values(by='tokens_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4da07122",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_ID = text_vocab['<unk>']\n",
    "BOS_ID = text_vocab['<bos>']\n",
    "EOS_ID = text_vocab['<eos>']\n",
    "text_vocab.set_default_index(UNK_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb680395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(sorted_token_df):\n",
    "    ids_data = []\n",
    "    for index, row in tqdm.tqdm(sorted_token_df.iterrows()):\n",
    "        text_ids = [BOS_ID]\n",
    "        for ttoken in row['text_tokens']:\n",
    "            text_ids.append(text_vocab[ttoken])\n",
    "        text_ids.append(EOS_ID)\n",
    "        highlight_ids = [BOS_ID]\n",
    "        for htoken in row['highlight_tokens']:\n",
    "            highlight_ids.append(text_vocab[htoken])\n",
    "        highlight_ids.append(EOS_ID)\n",
    "        ids_data.append((text_ids, highlight_ids))\n",
    "    return ids_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87592e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39515it [00:25, 1573.20it/s]\n",
      "1982it [00:01, 1609.13it/s]\n"
     ]
    }
   ],
   "source": [
    "ids_train = get_ids(sorted_train_df)\n",
    "ids_test = get_ids(sorted_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "656176ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    text_data = []\n",
    "    target_data = []\n",
    "    for unit in data:\n",
    "        text_data.append(torch.tensor(unit[0]))\n",
    "        target_data.append(torch.tensor(unit[1]))\n",
    "    text = pad_sequence(text_data, batch_first=True)\n",
    "    target = pad_sequence(target_data, batch_first=True)\n",
    "    return text, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a740fbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(ids_train, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(ids_test, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b46064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_data_loader, 'LSTM_data/train_data_loader.pth')\n",
    "torch.save(test_data_loader, 'LSTM_data/test_data_loader.pth')\n",
    "torch.save(text_vocab, 'LSTM_data/text_vocab.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86d49fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
